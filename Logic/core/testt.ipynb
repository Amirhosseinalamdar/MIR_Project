{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['amir hossein khubi chetori', 'chetori amir hossein khubi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amir hossein', 'hossein khubi', 'khubi chetori'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shingle_document(string, k=2):\n",
    "    shingles = set()\n",
    "    doc = string.split()\n",
    "    for i in range(len(doc) - k + 1):\n",
    "        shingles.add(' '.join(doc[i:i+k]))\n",
    "    return shingles\n",
    "shingle_document(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' c',\n",
       " ' h',\n",
       " ' k',\n",
       " 'am',\n",
       " 'bi',\n",
       " 'ch',\n",
       " 'ei',\n",
       " 'et',\n",
       " 'he',\n",
       " 'ho',\n",
       " 'hu',\n",
       " 'i ',\n",
       " 'in',\n",
       " 'ir',\n",
       " 'kh',\n",
       " 'mi',\n",
       " 'n ',\n",
       " 'or',\n",
       " 'os',\n",
       " 'r ',\n",
       " 'ri',\n",
       " 'se',\n",
       " 'ss',\n",
       " 'to',\n",
       " 'ub'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shingles = set()\n",
    "k=2\n",
    "for i in range(len(docs[0]) - k + 1):\n",
    "    shingles.add(docs[0][i:i + k])\n",
    "\n",
    "shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "shingles_lst = []\n",
    "all_shingles = set()\n",
    "for doc in docs:\n",
    "    shingles = shingle_document(doc)\n",
    "    # print(shingles)\n",
    "    all_shingles = all_shingles.union(shingles)\n",
    "    shingles_lst.append(shingles)\n",
    "\n",
    "N = len(all_shingles)\n",
    "M = len(docs)\n",
    "matrix = np.zeros((N, M), dtype=int)\n",
    "\n",
    "for i, sh in enumerate(all_shingles):\n",
    "    for j, shingles_set in enumerate(shingles_lst):\n",
    "        if sh in shingles_set:\n",
    "            matrix[i, j] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3145728 12582912 16777216 134217728 8388608 134217728\n",
      "309329920\n"
     ]
    }
   ],
   "source": [
    "res1 = 3*256*256*4*4\n",
    "res2 = 64*3*256*256\n",
    "res3 = 64*128*128*4*4\n",
    "res4 = 128*64*128*128\n",
    "res5 = 128*64*64*16\n",
    "res6 = 256*128*64*64\n",
    "\n",
    "print(res1, res2, res3, res4, res5, res6)\n",
    "print(res1 + res2 + res3 + res4 + res5 + res6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201326592 2147483648 2147483648\n",
      "4496293888\n"
     ]
    }
   ],
   "source": [
    "r1 = 3*4*4*64*256*256\n",
    "r2 = 64*4*4*128*128*128\n",
    "r3 = 128*4*4*256*64*64\n",
    "print(r1, r2, r3)\n",
    "print(r1+r2+r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a sample text with punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Define a regular expression pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'  # Matches any character that is not a word character or whitespace\n",
    "    \n",
    "    # Replace punctuation with an empty string\n",
    "    clean_text = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, world! This is a sample text with punctuation.\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a sample text with punctuation\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define punctuation characters\n",
    "    punctuation_chars = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    # Remove punctuation characters from the text\n",
    "    clean_text = ''.join(char for char in text if char not in punctuation_chars)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, world! This is a sample text with punctuation.\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amir     bi  22  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocess\n",
    "docs = ['amir hossein khubi @iran www.miow.comchetori']\n",
    "p = preprocess.Preprocessor(docs)\n",
    "p.remove_links('amir howww.ssein khu@ bi  22 a@iranwww.miow.comche&tori')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'are', 'chasing', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = text.split()  # Split text by whitespaces\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are chasing mice\"\n",
    "lemmatized_words = lemmatize_text(text)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello  wor ld  This is a sample text with punctuation \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_punctuation_with_whitespace(text):\n",
    "    # Define a regular expression pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'  # Matches any character that is not a word character or whitespace\n",
    "    \n",
    "    # Replace punctuation with whitespace\n",
    "    clean_text = re.sub(punctuation_pattern, ' ', text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, wor!ld! This is a sample text with punctuation.\"\n",
    "cleaned_text = replace_punctuation_with_whitespace(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'documents'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class Indexes(Enum):\n",
    "    DOCUMENTS = 'documents'\n",
    "    STARS = 'stars'\n",
    "    GENRES = 'genres'\n",
    "    SUMMARIES = 'summaries'\n",
    "\n",
    "Indexes.DOCUMENTS.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 51, 38]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_columns(arr):\n",
    "    norms = np.linalg.norm(arr)\n",
    "    normalized_arr = arr / norms\n",
    "    return normalized_arr\n",
    "\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 4],\n",
    "                 [7, 8, 5]])\n",
    "data2 = np.array([2, 3, 4])\n",
    "\n",
    "(data2 @ data).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fighter and me']\n"
     ]
    }
   ],
   "source": [
    "from preprocess import Preprocessor\n",
    "\n",
    "docs = ['fighter and me']\n",
    "print(Preprocessor(docs).preprocess())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/amirhossein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caring']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "tokens=['caring']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of 'fighter' is 'fighter'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem a word\n",
    "word = \"fighter\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "\n",
    "print(f\"The stemmed form of '{word}' is '{stemmed_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mieo man'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = 'mieo-man'\n",
    "punctuation_pattern = r'[^\\w\\s]'  \n",
    "re.sub(punctuation_pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "8 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lst \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: 8 is not in list"
     ]
    }
   ],
   "source": [
    "lst = [3, 4, 5]\n",
    "lst.index(8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
